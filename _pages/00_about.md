---
layout: about
permalink: /
title: Workshop on Human Evaluation of Generative Models
description: With rapid advances in generative models for both language and vision modalities, such as GPT-3, DALL-E, CLIP, and OPT, human evaluation of these systems is critical to ensure that they are meaningful, reliable, and aligned with the values of those who need them. These human evaluations are often trusted as indicators of whether models are safe enough to deploy, so it is important that these evaluations themselves are reliable. Several applications relying on these models have since emerged. Aside from the private sector, even governments are increasingly using generative models such as chatbots to better serve their citizens. However, the community also faces a lack of clarity around how to best conduct human evaluations (and what to even evaluate for). It is thus unclear whether prior established practices are sufficient given the socio-technical challenges posed by these systems. Recognizing the successes and socio-technical challenges associated with these technologies, this workshop aims to bring together researchers, practitioners, policy thinkers and implementers, and philanthropic funders to discuss major challenges, outline recent advances, and facilitate future research in these areas. 

profile:
  align: right
  image: 
  address: 
news: true
social: true
organizers: true
---

#### Topics of interest for submission include but are not limited to the following:

  <table style="background-color: #FAFAFA;">
        <col width="40">
        <col width="100">
        <tr style="border: none;">       
            <td style="border: none;">
            <b> <font size="+0" color="purple">Experimental design and methods for human evaluations</font> </b>
            </td>
            <td style="border: none;">
            <i> Role of human evaluation in the context of value alignment of large generative models</i>   
            </td>                        
        </tr>
        <tr style="border: none;">    
        <td style="border: none;">
            <b> <font size="+0" color="purple">Interpretability</font></b> 
            </td>
            <td style="border: none;">
            <i> Role of humans in building trustworthy AI systems: model interpretability and algorithmic fairness.</i>
            </td>  
        </tr>
        <tr style="border: none;">
        <td style="border: none;">
            <b> <font size="+0" color="purple">Human as Model Adversary</font></b>
            </td>
            <td style="border: none;">
            <i> Designing testbeds for evaluating generative models</i>
            </td>
        </tr>
        <tr style="border: none;">
        <td style="border: none;">
            <b> <font size="+0" color="purple">System Design</font></b>
            </td>
            <td style="border: none;">
            <i> Reproducibility of human evaluations</i>
            </td>
        </tr>
        <tr style="border: none;">
        <td style="border: none;">
            <b> <font size="+0" color="purple">Model Evaluation</font></b>
            </td>
            <td style="border: none;">
            <i> Ethical considerations in human evaluation of computational systems</i>
            </td>
        </tr>
        <tr style="border: none;">
            <td style="border: none;">
            <b><font size="+0" color="purple">Crowdsourcing</font></b>
            </td>
            <td style="border: none;">
            <i> Issues in meta-evaluation of automatic metrics by correlation with human evaluations</i>
            </td>
        </tr>
        <tr style="border: none;">
            <td style="border: none;">
            <b><font size="+0" color="purple">Crowdsourcing</font></b>
            </td>
            <td style="border: none;">
            <i> Methods for assessing the quality and the reliability of human evaluations</i>
            </td>
        </tr>
  </table>   




<div style="line-height:40%;">
    <br>
</div>

