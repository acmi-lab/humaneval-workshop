---
layout: about
permalink: /
title: Workshop on Human Evaluation of Generative Models
description: With rapid advances in generative models for both language and vision modalities, such as GPT-3, DALL-E, CLIP, and OPT, human evaluation of these systems is critical to ensure that they are meaningful, reliable, and aligned with the values of those who need them. These human evaluations are often trusted as indicators of whether models are safe enough to deploy, so it is important that these evaluations themselves are reliable. Several applications relying on these models have since emerged. Aside from the private sector, even governments are increasingly using generative models such as chatbots to better serve their citizens. However, the community also faces a lack of clarity around how to best conduct human evaluations (and what to even evaluate for). It is thus unclear whether prior established practices are sufficient given the socio-technical challenges posed by these systems. Recognizing the successes and socio-technical challenges associated with these technologies, this workshop aims to bring together researchers, practitioners, policy thinkers and implementers, and philanthropic funders to discuss major challenges, outline recent advances, and facilitate future research in these areas. 

profile:
  align: right
  image: 
  address: 
news: true
social: true
organizers: true
---

#### Key Dates 

* Submission Deadline: September 15th, 23:59 GMT
* Accept/Reject notifications: October 20th
* Workshop: December 3rd, in person in New Orleans

#### Topics of interest for submission include but are not limited to the following:

- Experimental design and methods for human evaluations
- Role of human evaluation in the context of value alignment of large generative models
- Designing testbeds for evaluating generative models
- Reproducibility of human evaluations
- Ethical considerations in human evaluation of computational systems
- Issues in meta-evaluation of automatic metrics by correlation with human evaluations
- Methods for assessing the quality and the reliability of human evaluations
  




<div style="line-height:40%;">
    <br>
</div>

